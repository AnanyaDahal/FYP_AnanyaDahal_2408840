# -*- coding: utf-8 -*-
"""Data Cleaning and Merging and training random forest model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QpAm6xF9Ww5MF93LYEkpVpJDNVqzj6xg
"""

import os
import pandas as pd
import numpy as np

DATASET_DIR = "/content/drive/MyDrive/Datasets_for_fyp"
csv_files = [f for f in os.listdir(DATASET_DIR) if f.endswith(".csv")]

print("CSV files found:", csv_files)

import pandas as pd

for file in csv_files:
    df_temp = pd.read_csv(os.path.join(DATASET_DIR, file))
    print("\n==============================")
    print(f"File: {file}")
    print("Columns:", df_temp.columns.tolist())
    print("Shape:", df_temp.shape)

"""load and standardize datasets

"""

for file in csv_files:
    try:
        df_temp = pd.read_csv(os.path.join(DATASET_DIR, file), header=0, index_col=False)

        # 1. Immediate cleanup
        df_temp = df_temp.loc[:, ~df_temp.columns.duplicated()].copy()
        df_temp.columns = df_temp.columns.str.lower().str.strip()

        # 2. Standardize URL (Handle multiple potential matches)
        url_cols = ["url", "website", "domain", "websites"]
        found_url = [c for c in url_cols if c in df_temp.columns]
        if found_url:
            # Rename the first match to 'url' and drop others if they exist
            df_temp.rename(columns={found_url[0]: "url"}, inplace=True)

        # 3. Standardize Label
        label_cols = ["label", "class", "result", "type", "status"]
        found_label = [c for c in label_cols if c in df_temp.columns]
        if found_label:
            df_temp.rename(columns={found_label[0]: "label"}, inplace=True)

        # 4. Filter and Finalize
        if "url" in df_temp.columns and "label" in df_temp.columns:
            # Keep only the first occurrence of 'url' and 'label' if duplicates exist
            df_temp = df_temp.loc[:, ~df_temp.columns.duplicated()]
            df_temp = df_temp[["url", "label"]].copy()
            df_temp = df_temp.dropna(subset=["url", "label"])

            # Reset index to ensure it's unique before adding to list
            df_temp = df_temp.reset_index(drop=True)
            dfs.append(df_temp)

    except Exception as e:
        print(f"Skipping file {file} due to error: {e}")

# Create a clean list for finalized dataframes
clean_dfs = []

for temp_df in dfs:
    # 1. Ensure no duplicate column names exist within the individual dataframe
    # This keeps the first occurrence of 'url' or 'label' and drops others
    temp_df = temp_df.loc[:, ~temp_df.columns.duplicated()].copy()

    # 2. Reset the index of the individual dataframe
    temp_df = temp_df.reset_index(drop=True)

    clean_dfs.append(temp_df)

# Combine all datasets
if clean_dfs:
    df = pd.concat(clean_dfs, axis=0, ignore_index=True)
    print("Combined dataset shape:", df.shape)

    # Show unique labels to check for inconsistencies
    print("\nUnique labels found in combined data:")
    print(df['label'].unique())
else:
    print("No valid datasets found to combine.")

print(df['label'].value_counts())

# Standardizing labels to 0 (benign) and 1 (phishing)
label_mapping = {
    'benign': 0, 'safe': 0, 'legitimate': 0, '0': 0, 0: 0, 'good': 0,
    'phishing': 1, 'malicious': 1, 'bad': 1, '1': 1, 1: 1, 'spam': 1
}

df['label'] = df['label'].map(label_mapping)

# Drop any rows that didn't match our mapping (if any)
df = df.dropna(subset=['label'])
df['label'] = df['label'].astype(int)

# Remove duplicate URLs
df = df.drop_duplicates(subset=['url']).reset_index(drop=True)

# Shuffle the data (important for training later)
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

print(f"Final dataset size: {df.shape}")

df.to_csv("/content/drive/MyDrive/Datasets_for_fyp/cleaned_combined_dataset.csv", index=False)

!pip install tldextract

import re
import tldextract

# Helper functions
def url_length(url): return len(str(url))
def count_dots(url): return str(url).count('.')
def count_hyphens(url): return str(url).count('-')
def count_slashes(url): return str(url).count('/')
def count_at(url): return str(url).count('@')
def count_question(url): return str(url).count('?')
def count_equal(url): return str(url).count('=')
def digit_count(url): return sum(c.isdigit() for c in str(url))
def letter_count(url): return sum(c.isalpha() for c in str(url))
def count_special_chars(url): return len(re.findall(r"[^A-Za-z0-9]", str(url)))
def has_ip_address(url):
    ip_pattern = r"\b(?:\d{1,3}\.){3}\d{1,3}\b"
    return 1 if re.search(ip_pattern, str(url)) else 0
def https_token(url): return 1 if str(url).startswith("https") else 0
def is_encoded(url): return 1 if "%" in str(url) else 0
def extract_domain(url):
    try: return tldextract.extract(str(url)).registered_domain
    except: return ""
def extract_subdomain(url):
    try: return tldextract.extract(str(url)).subdomain
    except: return ""
def domain_length(url): return len(extract_domain(url))
def subdomain_length(url): return len(extract_subdomain(url))
def has_multiple_subdomains(url): return 1 if extract_subdomain(url).count('.') >= 2 else 0
def suspicious_words(url):
    keywords = ["secure", "account", "login", "update", "verify", "bank", "free", "lucky"]
    return sum(1 for k in keywords if k in str(url).lower())
def count_params(url): return str(url).count('&')
def has_shortening_service(url):
    services = ["bit.ly", "tinyurl", "goo.gl", "t.co", "ow.ly"]
    return 1 if any(s in str(url).lower() for s in services) else 0
def get_tld(url):
    try: return tldextract.extract(str(url)).suffix
    except: return ""
def tld_length(url): return len(get_tld(url))

# Extract features for all URLs
feature_data = {
    "url_length": df["url"].apply(url_length),
    "count_dots": df["url"].apply(count_dots),
    "count_hyphens": df["url"].apply(count_hyphens),
    "count_slash": df["url"].apply(count_slashes),
    "count_at": df["url"].apply(count_at),
    "count_question": df["url"].apply(count_question),
    "count_equal": df["url"].apply(count_equal),
    "digit_count": df["url"].apply(digit_count),
    "letter_count": df["url"].apply(letter_count),
    "special_chars": df["url"].apply(count_special_chars),
    "has_ip": df["url"].apply(has_ip_address),
    "https_flag": df["url"].apply(https_token),
    "encoded_url": df["url"].apply(is_encoded),
    "domain_length": df["url"].apply(domain_length),
    "subdomain_length": df["url"].apply(subdomain_length),
    "multiple_subdomains": df["url"].apply(has_multiple_subdomains),
    "suspicious_words": df["url"].apply(suspicious_words),
    "count_params": df["url"].apply(count_params),
    "shortening_service": df["url"].apply(has_shortening_service),
    "tld_length": df["url"].apply(tld_length)
}

feature_df = pd.DataFrame(feature_data)
print("Feature extraction complete. Shape:", feature_df.shape)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib

# Split data
X = feature_df
y = df["label"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

#Scale features (optional)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Random Forest
clf = RandomForestClassifier(n_estimators=200, random_state=42)
clf.fit(X_train_scaled, y_train)

# Evaluate
y_pred = clf.predict(X_test_scaled)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Save model, scaler, and features
joblib.dump(clf, "/content/drive/MyDrive/Datasets_for_fyp/rf_model.pkl")
joblib.dump(scaler, "/content/drive/MyDrive/Datasets_for_fyp/scaler.pkl")
joblib.dump(feature_df.columns.tolist(), "/content/drive/MyDrive/Datasets_for_fyp/features.pkl")
print("Model, scaler, and feature list saved.")

